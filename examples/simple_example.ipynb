{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiple_evaluations(\n",
    "        path_to_gt: str,\n",
    "        paths_to_predictions: list[str],\n",
    "        path_to_seq_ids: str,\n",
    "        labels_enum,\n",
    "        classes_to_eval: list,\n",
    "        metrics_to_eval: list\n",
    "):\n",
    "    \"\"\"\n",
    "        Benchmarks multiple prediction files against a ground truth and generates comparative plots.\n",
    "\n",
    "    Args:\n",
    "        path_to_gt: Path to the ground truth HDF5 file.\n",
    "        paths_to_predictions: A list of paths to prediction HDF5 files.\n",
    "        path_to_seq_ids: Path to a .npy file containing sequence IDs for benchmarking.\n",
    "        labels_enum: Enum defining data labels (e.g., BendLabels).\n",
    "        classes_to_eval: List of class enums to evaluate (e.g., [BendLabels.EXON]).\n",
    "        metrics_to_eval: List of metric enums to evaluate (e.g., [EvalMetrics.INDEL]).\n",
    "    :param path_to_gt:\n",
    "    :param paths_to_predictions:\n",
    "    :param path_to_seq_ids:\n",
    "    :param labels_enum:\n",
    "    :param classes_to_eval:\n",
    "    :param metrics_to_eval:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    for pred_path in paths_to_predictions:\n",
    "        reader = H5Reader(path_to_gt=path_to_gt, path_to_predictions=pred_path)\n",
    "        benchmark_results = benchmark_all(\n",
    "            reader=reader,\n",
    "            path_to_ids=path_to_seq_ids,\n",
    "            labels=labels_enum,\n",
    "            classes=classes_to_eval,\n",
    "            metrics=metrics_to_eval\n",
    "        )\n",
    "        # Extract method name from file path\n",
    "        method_name = pred_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        all_results[method_name] = benchmark_results\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def benchmark_all(reader: H5Reader, path_to_ids: str, labels, classes, metrics, collect_individual_results: bool = False):\n",
    "    ids = np.load(path_to_ids)\n",
    "    gts = []\n",
    "    preds = []\n",
    "\n",
    "    for seq_id in tqdm(ids,desc=\"Loading sequence labels\"):\n",
    "        bend_annot_forward, bend_annot_reverse = reader.get_gt_pred_pair(seq_id)\n",
    "\n",
    "        gts.append(bend_annot_forward[0])\n",
    "        preds.append(bend_annot_forward[1])\n",
    "        gts.append(bend_annot_reverse[0])\n",
    "        preds.append(bend_annot_reverse[1])\n",
    "\n",
    "    return benchmark_gt_vs_pred_multiple(gt_labels=gts, pred_labels=preds, labels=labels, classes=classes, metrics=deepcopy(metrics),\n",
    "                                         collect_individual_results=collect_individual_results)"
   ],
   "id": "f5844defe651135e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
